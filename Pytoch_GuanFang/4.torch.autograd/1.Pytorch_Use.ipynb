{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pytorch的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从torchvision. 我们创建一个随机数据张量来表示具有 3 个通道、高度和宽度为 64 的单个图像，并将其对应的label初始化为一些随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 resnet18的模块，不懂没关系，我们重要的是学会整套“游泳动作”\n",
    "model = torchvision.models.resnet18(pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch, channel, height, width]，表示batch_size=1(灰度图),3通道，图像尺寸64x64 的一个Tensor\n",
    "data = torch.rand(1,3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.rand(1, 1000) # 表示1维的Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们通过模型的每一层运行输入数据以进行预测。这是前传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用模型的预测和相应的标签来计算误差 ( loss)。下一步是通过网络反向传播这个误差。当我们调用.backward()误差张量时，反向传播就开始了。Autograd 然后计算每个模型参数的梯度并将其存储在参数的.grad属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们加载一个优化器，在这种情况下，SGD 的学习率为 0.01，动量为 0.9。我们在优化器中注册模型的所有参数。SGD：随机梯度下降法。具体介绍见torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们调用.step()启动梯度下降。优化器通过存储在 中的梯度来调整每个参数.grad。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc4074118cc46b1f5224fd9371a8e74610038dc54e2a6a6af6fd5f5aec7d4206"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('Pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
